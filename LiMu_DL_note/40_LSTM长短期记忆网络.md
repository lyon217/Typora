# 40_LSTM

![image-20220908151745921](40_LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C.assets/image-20220908151745921.png)

![image-20220908151910249](40_LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C.assets/image-20220908151910249.png)

## 候选记忆单元

![image-20220908151926313](40_LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C.assets/image-20220908151926313.png)

$\tilde{C_t}$没有用到任何gate

## 记忆单元

![image-20220908152605843](40_LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C.assets/image-20220908152605843.png)

上一时刻的记忆单元会作为状态放进来，这是LSTM跟RNN不一样的一点，就是他有两个状态，一个是C,一个是H,这也是为什么RNN从零实现的的forward_fn的函数返回的值是输出的值加上(H, )空出来的这个位置就是C

其实根据$C_t$的计算公式可以看出，如果F变为0的话，那说明就是我尽量不要去记住$C_{t-1}$,同理，如果$I_t$等于0的话，那就是把现在的记忆单元的东西丢掉，有点点像我们之前计算GRU中的计算$H_t$的公式，但是$H_t$里面是z和1-z，就是如果一个有那么另一个就是相反的，那LSTM中的这里可以理解为前后的$C$是否要使用是独立来决定的。

## 隐状态

![image-20220908154548189](40_LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C.assets/image-20220908154548189.png)

**<u>$H_t$的计算中又对我们的候选记忆单元$C_t$做了一个tanh,是因为在记忆单元$C_t$的计算过程中，是使用两部分的和来进行计算的，而这两部分的和可能导致数值过大，所以又做了一次tanh，将两部分的和映射到(-1,1)之间</u>**

$O_t$决定的是我要不要输出，如果趋近于0就是不要输出了，意思就是当前的$X_t$和过去的所有信息我都不要，也可以说是直接重置了。

## 总结

![image-20220908154943973](40_LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C.assets/image-20220908154943973.png)

相比GRU多了一个C,这个C可以说是辅助的记忆单元
