## 背景

![image-20220912154229888](47_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20220912154229888.png)

不随意：潜意识 (可以理解为不跟随自己的意愿无意间看到的内容)

随意：主动  (可以理解为跟随自己的意愿看到的内容)

## 注意力机制

![image-20220912154633448](47_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20220912154633448.png)

- 卷积全连接池化层都是不随意的，因为他们不在意我需要什么，就像是卷积就是把显示的大量pixel抽取出来，池化层就是找到最大的
- 注意力机制就是考虑我想要什么，去寻找随意线索
- 随意线索被称之为Query就是可以代指我想要的内容
- 每个输入是一个值Keys和不随意线索values的pair,可以理解为环境，也可以理解为物体本身的属性与对我的价值的对应关系
- Attention pooling会根据我们的Query来有偏向的选择某一些输入，也就是key value pair,跟之前的池化层有所不同，这里显示的加入了query，需要去查询我们的随意线索，代表着我们想要去干嘛

## 非参注意力池化层

![image-20220912155800370](47_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20220912155800370.png)

- x->key, y->value

- 此处的$f(x)$就是query，就是我们想要得到的内容，那么最原始的获得Query的方法就是对所有的y取平均

- Nadaraya-Watson中的K代表的就是一个核函数，离参照点x越进则K(x-xi)的值越大，这就相当于一个权值，然后再乘以y，相当于对y进行加权，会发现如果xi离x越近权重会越大，就越重要，越有参考价值

## Nararaya-Watson核回归

![image-20220912161611172](47_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20220912161611172.png)

- 核：K函数，可以看做是衡量x和xi之间距离的函数

- u:代表x和xi之间的距离

可以看到，如果K使用的是高斯核函数，那么结果将可以转变为softmax，然后softmax将所有x与xi之间的距离映射到0-1的一个数字上对yi进行加权，到目前为止，得到的这个结果已经跟我们的注意力机制非常非常像了，

数据就是给定的数据，对于新给定的值来讲，只需要在给定的数据中进行查询就可以了，（选择和新给定的值比较相接近的数据，然后将这些数据对应的value值进行加权就和得到最终的query）所以不需要学习参数

## 参数化的注意力机制

![image-20220912163124383](47_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20220912163124383.png)

w是一个可学习的参数	

## 总结

![image-20220912163205420](47_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20220912163205420.png)