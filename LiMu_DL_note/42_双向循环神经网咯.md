讲序列模型的时候就讲过，一个序列可以从前往后看，当然在某些情况下也可以从后往前看，当然某些情况下是不可以的

![image-20220908162104800](42_%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E5%92%AF.assets/image-20220908162104800.png)

## 双向网络

隐藏层有两个，一个是前向，一个是反向，

其实实现很简单，前向就是正常的实现，反向的话就直接将序列反向一下然后再代入就可以

![image-20220908163237965](42_%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E5%92%AF.assets/image-20220908163237965.png)

## 推理

![image-20220908163322228](42_%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E5%92%AF.assets/image-20220908163322228.png)

**<u>训练的时候是方便的，因为某一个词元的前后的序列我们都能拿到，但是做推理的时候我们只能拿到前面的，所以双向循环神经网络非常不适合做推理，但是非常适合去对一个句子做特征提取</u>**

## 总结

![image-20220908163744177](42_%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E5%92%AF.assets/image-20220908163744177.png)

# QA

1. LSTM为什么要分C和H

    H就是-1到1之间的数字，但是C可以做的比较大，可以用来存储信息，是没有数值限制的

2. 双向循环神经网络，在正向和反向之间有权重关系没？

    是没有任何关系的，二者做的不是+，而是concat

3. 隐藏层和隐马尔科夫有什么关系？

​	理论上的RNN是不需要要隐马尔科夫的假设的，只是在训练的时候我们把句子裁成一定长度然后放进去训练，这里其实是暗含着我们用了一个隐马尔科夫的假设，其实二者没有什么关系

4. 是否可以深度双向？

    可以,就是在nn.LSTM()里的参数bidirectional设置为True的时候，网络则是双向LSTM