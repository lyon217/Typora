# 39_门控循环单元GRU

**RNN中的问题：**

RNN中无法处理太长的序列，因为要把整个序列信息全部放到一个隐藏状态里面，当时间很长的时候，可能隐藏状态里累计了太多东西了，这个时候对于比较前面的信息可能就不那么好去从里面抽取出来了。

所以GRU就是通过一些门控逻辑单元来使得我们在构造隐藏状态的时候取选择重要的看一看，不重要的不主要看。

![image-20220908093013968](39_GRU%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83.assets/image-20220908093013968.png)

update gate 更新门：判断哪些比较重要的状态，尽量要用这些去更新我的隐藏状态，就是尽量的将有用的信息放到更新门里，然后交给后面的隐藏状态

reset gate 重置门：将隐藏状态中的一些东西忘掉，（更新门将允许我们控制新状态中有多少个是旧状态的副本。）

## 门

![image-20220908094240437](39_GRU%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83.assets/image-20220908094240437.png)

$R_t$Reset使用来说，我在更新我新隐藏状态的时候，要用到多少过去的隐藏状态的信息

门就是跟我们的隐藏状态长度是一样的一个向量，因为后面要做按元素乘法来决定隐藏状态的值，其实也就可以理解为门就是隐藏状态的权重。

经过两个公式可以发现二者无论是大小还是计算方式都是完全一致的，所以二者的区别实际是在再往下一层，对于候选隐变量和真正隐变量的计算的不同，那么在进行学习的时候，二者就会一个更偏向于记录要记住的，一个更偏向于记录要忘记的。

## 候选隐状态$\tilde{H_t}$

![image-20220908094320155](39_GRU%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83.assets/image-20220908094320155.png)

这里发现我们的$\tilde{H_t}$其实只跟$R_t$有关，当$R_t$全是0的时候，就相当于忘记之前的状态，从头开始，当$R_t$全是1的时候，就相当于原始的RNN，因为是可以学习的，所以可以自动去控制着根据当前的内容去决定之前的哪些内容我就不看了

## (真正的)隐状态

![image-20220908094454523](39_GRU%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83.assets/image-20220908094454523.png)

所以$H_t$其实可以看做是$H_{t-1}$和$\tilde{H_t}$的加权平均？

假设Z_t非常趋近1，就相当于当前的H_t直接使用的是H_{t-1}，也相当于就是说直接舍弃掉了当前的x，不用当前的x来更新我的隐变量的值，等于是说忽略掉了当前的x的元素

假设Z_t接近0，就基本回到了RNN的情况，相当于基本不去拿过去的状态了，只看现在的状态

## 总结

![image-20220908105216692](39_GRU%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83.assets/image-20220908105216692.png)

![image-20220908110947913](39_GRU%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83.assets/image-20220908110947913.png)

**个人理解：**

R的作用：用我的H_{t-1}和X_t去更新我的H_{t-1}的时候，看多少H_{t-1}的信息，就是要不要根据我的X_t来update我的H_t

Z的作用：有多少概率直接绕过X_t了，直接去用H_{t-1}的信息,就是是不是把过去的信息都不要都忘掉

# QA

1. rnn多长算长，还是跟rnn的隐藏向量大小有关？

    没办法说一个具体的数字算是多长，但是实际情况下不要用rnn，用gru或者是lstm都是可以的，当然LM老师的建议是gru或者是lstm的长度最好不要超过100，当然如果要做到1000的话，可以考虑最后的transformer或者BERT
