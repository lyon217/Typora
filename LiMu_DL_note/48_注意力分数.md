## 注意力分数

![image-20220913165209157](48_%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0.assets/image-20220913165209157.png)

![image-20220913165304744](48_%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0.assets/image-20220913165304744.png)

- q,k,v的shape都可以不一样

## Additive Attention 可加性注意力

![image-20220913165449041](48_%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0.assets/image-20220913165449041.png)

- <u>常用于k，v,q是不一样的长度的情况</u>
- 可学习的参数$W_k\in \mathbb{R}^{h×k}$就是将k map到h的矩阵，$W_q$同理，V是一个向量

- 通过$W_kk$得到长为h的向量然后加上$W_qq$的长为h的向量，然后再激活，然后乘以$V^T$，最终得到的就是一个值，等价于将key和value合并起来放到一个隐藏层为h输出大小为1的单隐藏层MLP

## Scaled Dot-Product Attention 缩放点积

![image-20220913172625764](48_%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0.assets/image-20220913172625764.png)

+ 一般用于key和query都是同样长度的

- 这种情况下，就不学东西了，就直接q和k做内积然后除以根号d  

## 总结

![image-20220913202336275](48_%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0.assets/image-20220913202336275.png)